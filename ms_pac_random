#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Feb 20 21:09:59 2018

@author: bruceokallau
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sat Jan 27 22:54:54 2018

@author: bruceokallau
"""
import time
import numpy as np
import gym


start_time = time.time()

env = gym.make('MsPacman-ram-v0')
''' ms pacman ram gives us the 128 byte RAM of the Atari game, possible actions range from 0-9?'''

total_episodes = 500 #Set total number of episodes to train agent on.
max_ep = 999

i = 0
total_reward = []
total_lenght = []

        
while i < total_episodes:
    s = env.reset()
    env.render()
    running_reward = 0
    ep_history = []
    for j in range(max_ep):            
        a = env.action_space.sample()

        s1,r,d,l = env.step(a) #Get our reward for taking an action
        env.render()
        ep_history.append([s,a,r,s1,l])
        s = s1
        running_reward += r
        if d == True:
            total_reward.append(running_reward)
            total_lenght.append(j)
            break

        
            #Update our running tally of scores.
    if i % 100 == 0:
        print(np.mean(total_reward[-100:]))
    i += 1

print("%d episodes %f seconds - mean total reward %f" % (total_episodes, (time.time() - start_time) , (np.mean(total_reward))))

# 500 episodes 785.527534 seconds - mean total reward 210.121457
# 100 ep - 125.149722 seconds - mean total reward - 211.326530612
# 10 ep - 43 seconds - mean total reward = 194

# should next run for 1000, 2000, 4000 and graph results, maybe even predict results for 5000, then run 5000
